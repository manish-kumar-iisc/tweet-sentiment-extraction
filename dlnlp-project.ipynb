{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n/kaggle/input/tweet-sentiment-extraction/train.csv\n/kaggle/input/tweet-sentiment-extraction/test.csv\n/kaggle/input/bert-base-uncased/config.json\n/kaggle/input/bert-base-uncased/pytorch_model.bin\n/kaggle/input/bert-base-uncased/vocab.txt\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install ktrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All imports"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom torch.utils.data import TensorDataset, DataLoader\nimport re\nimport string\nimport transformers\nimport torch.optim as optim\nfrom tqdm.autonotebook import tqdm\nfrom sklearn.metrics import f1_score,precision_score,recall_score\n","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\").dropna().reset_index(drop=True)\ntest=pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\").dropna().reset_index(drop=True)\nsample=pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\").dropna().reset_index(drop=True)","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data path is .csv file with 2 columns, having name 'text', 'sentiment'.\n# test is also .csv file with 1 column, name 'text' only\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\nos.environ[\"VISIBLE_DEVICES\"]=\"0\"\nimport ktrain\nfrom ktrain import text\nDATA_PATH = '../input/tweet-sentiment-extraction/train.csv'\nNUM_WORDS = 25000\nMAXLEN = 128\n(x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n                      'text',\n                      label_columns = [\"sentiment\"],\n                      val_filepath='../input/tweet-sentiment-extraction/test.csv', # if None, 10% of data will be used for validation\n                      max_features=NUM_WORDS, maxlen=MAXLEN,\n                      ngram_range=1,preprocess_mode='bert')\nmodel=text.text_classifier('bert',(x_train,y_train),preproc=preproc)\nlearner=ktrain.get_learner(model,(x_train,y_train),(x_test,y_test),batch_size=32)\nlearner.fit(3e-5,3,cycle_len=1,cycle_mult=1)\n# learner.lr_find(max_epochs=3)\n# learner.lr_plot()\npredictor=ktrain.get_predictor(learner.model, preproc)\noutput=predictor.predict(np.array(test['text']))\ncount=0;\nfor i,samp in enumerate(output):\n    if samp==test['sentiment'].iloc[i]:\n        count+=1\nprint(f'accuracy on test data: {count/len(test)}')","execution_count":25,"outputs":[{"output_type":"stream","text":"detected encoding: utf-8 (if wrong, set manually)\npreprocessing train...\nlanguage: en\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"done."},"metadata":{}},{"output_type":"stream","text":"Is Multi-Label? False\npreprocessing test...\nlanguage: en\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"done."},"metadata":{}},{"output_type":"stream","text":"Is Multi-Label? False\nmaxlen is 128\ndone.\nEpoch 1/3\n859/859 [==============================] - 738s 859ms/step - loss: 0.6025 - accuracy: 0.7431 - val_loss: 0.5052 - val_accuracy: 0.7787\nEpoch 2/3\n859/859 [==============================] - 732s 852ms/step - loss: 0.4278 - accuracy: 0.8312 - val_loss: 0.5098 - val_accuracy: 0.7903\nEpoch 3/3\n859/859 [==============================] - 729s 849ms/step - loss: 0.3090 - accuracy: 0.8819 - val_loss: 0.5370 - val_accuracy: 0.7892\naccuracy on test data: 0.7891907187323146\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"[array([[ 101, 2130, 2295, ...,    0,    0,    0],\n        [ 101, 2204, 2000, ...,    0,    0,    0],\n        [ 101, 1045, 2081, ...,    0,    0,    0],\n        ...,\n        [ 101, 4283,  999, ...,    0,    0,    0],\n        [ 101, 2138, 1045, ...,    0,    0,    0],\n        [ 101, 1045, 2428, ...,    0,    0,    0]]),\n array([[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]])]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Distil bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=train['text'].values\nx_test=test['text'].values\ny_train=train['sentiment'].values\ny_test=test['sentiment'].values\ntrn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n                                          x_test=x_test, y_test=y_test,\n                                          class_names=[0,1,2],\n                                          preprocess_mode='distilbert',\n                                          maxlen=150)\ntext.print_text_classifiers()\nmodel = text.text_classifier('distilbert', train_data=trn, preproc=preproc)\nlearner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)\nlearner.fit_onecycle(3e-5, 4)\np = ktrain.get_predictor(model, preproc)","execution_count":18,"outputs":[{"output_type":"stream","text":"preprocessing train...\nlanguage: en\ntrain sequence lengths:\n\tmean : 13\n\t95percentile : 25\n\t99percentile : 28\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ktrain/utils.py:589: UserWarning: class_names argument was ignored, as they were extracted from string labels in dataset\n  if self.get_classes(): warnings.warn('class_names argument was ignored, as they were extracted from string labels in dataset')\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"stream","text":"Is Multi-Label? False\npreprocessing test...\nlanguage: en\ntest sequence lengths:\n\tmean : 13\n\t95percentile : 25\n\t99percentile : 28\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"stream","text":"task: text classification\nfasttext: a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]\nlogreg: logistic regression using a trainable Embedding layer\nnbsvm: NBSVM model [http://www.aclweb.org/anthology/P12-2018]\nbigru: Bidirectional GRU with pretrained fasttext word vectors [https://fasttext.cc/docs/en/crawl-vectors.html]\nstandard_gru: simple 2-layer GRU with randomly initialized embeddings\nbert: Bidirectional Encoder Representations from Transformers (BERT) from keras_bert [https://arxiv.org/abs/1810.04805]\ndistilbert: distilled, smaller, and faster BERT from Hugging Face transformers [https://arxiv.org/abs/1910.01108]\nIs Multi-Label? False\nmaxlen is 150\ndone.\n\n\nbegin training using onecycle policy with max lr of 3e-05...\nEpoch 1/4\n4580/4580 [==============================] - 380s 83ms/step - loss: 0.6196 - accuracy: 0.7375 - val_loss: 0.5362 - val_accuracy: 0.7827\nEpoch 2/4\n4580/4580 [==============================] - 378s 83ms/step - loss: 0.4825 - accuracy: 0.8074 - val_loss: 0.5158 - val_accuracy: 0.7849\nEpoch 3/4\n4580/4580 [==============================] - 378s 82ms/step - loss: 0.3536 - accuracy: 0.8646 - val_loss: 0.5458 - val_accuracy: 0.7949\nEpoch 4/4\n4580/4580 [==============================] - 380s 83ms/step - loss: 0.1651 - accuracy: 0.9429 - val_loss: 0.7190 - val_accuracy: 0.7881\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"p.predict(val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1macro=f1_score(output,test['sentiment'],average='macro')\nf1avg=f1_score(output,test['sentiment'],average='weighted')\nrecall=recall_score(output,test['sentiment'],average='macro')\nrecallw=recall_score(output,test['sentiment'],average='weighted')\nprecision=precision_score(output,test['sentiment'],average='macro')\nprecisionw=precision_score(output,test['sentiment'],average='weighted')\nprint(f'f1macro: {f1macro} recall: {recall} precision :{precision}')\nprint(\"below result for average\")\nprint(f'f1avg: {f1avg} recall: {recallw} precision :{precisionw}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q&A model supervised"},{"metadata":{"trusted":true},"cell_type":"code","source":"class QAModel(nn.Module):\n    \n    def __init__(self):\n        super(QAModel,self).__init__()\n        self.bert=transformers.BertForQuestionAnswering.from_pretrained(\"../input/bert-base-uncased\")\n        # make it correct\n#         self.dropout=nn.Dropout(0.2)\n#         self.layer1=nn.Linear(768*2,2)\n        self.softmax=nn.Softmax()\n    \n    def forward(self, ids,attn,token_type):\n        start,end=self.bert(ids,attention_mask=attn,token_type_ids=token_type)\n#         out=self.dropout(out2)\n#         out=self.layer1(out)\n#         start=self.softmax(start)\n#         end=self.softmax(end)\n        \n\n        return start, end\n        ","execution_count":95,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## unsupervised model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## coming soon","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training, evaluation, inference function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def training():\n    \n    \n# def evaluate():\n    \n\n# def inference():\n\n","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\d', '', text)\n    return text","execution_count":96,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* negative: 4997\n* positive: 3893\n* neutral:  8699\n* [CLS]   : 101\n* [SEP]   : 102"},{"metadata":{},"cell_type":"markdown","source":"## token_ids, attn_masks, token_type_ids, start & end logits"},{"metadata":{"trusted":true},"cell_type":"code","source":"token_ids=[]\nattn_masks=[]\ntoken_type_ids=[]\nstart_logits=[]\nend_logits=[]\ndef preprocess(data,typ):\n    \n    maxlen=115\n    tokenizer=transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n    \n    for i,_ in enumerate(range(len(data))):\n        \n        #finding start & end logits\n        out1=tokenizer.encode_plus(data['text'].iloc[i])\n        if typ=='train':\n            out2=tokenizer.encode_plus(data['selected_text'].iloc[i])\n            l=len(out2['input_ids'])\n        if typ=='train':\n            for i,_ in enumerate(out1['input_ids']):\n                if out1['input_ids'][i:i+l-2]==out2['input_ids'][1:-1]:\n                    break\n            start=i\n            end=i+l-2\n            temp=[0]*(maxlen+2)\n            temp[start]=1\n            start_logits.append(temp)\n            temp=[0]*(maxlen+2)\n            temp[end-1]=1\n            end_logits.append(temp)\n        #making length to maximum\n        if data['sentiment'].iloc[i]=='positive':\n            sentiment_id=4893\n        elif data['sentiment'].iloc[i]=='negative':\n            sentiment_id=4893\n        else:\n            sentiment_id=4893\n        tkn_len=len(out1['input_ids'])\n        out1['input_ids']=[101]+[sentiment_id]+[102]+out1['input_ids'][1:]+(maxlen-tkn_len)*[0]\n        token_ids.append(out1['input_ids'])\n        \n        token_type_ids.append(3*[1]+out1['token_type_ids']+(maxlen-tkn_len-1)*[1])\n        attn_masks.append(2*[1]+out1['attention_mask']+(maxlen-tkn_len)*[0])\n    if typ=='train':\n        data['start']=start_logits\n        data['end']=end_logits\n    data['token_ids']=token_ids\n    data['token_type']=token_type_ids\n    data['attn_masks']=attn_masks\n       \n    \n#     start_logits=[]\n#     end_logits=[]\n#     for i in range(len(train)):\n#         a=train['text'].iloc[i].index(train['selected_text'].iloc[i])\n#         start_logits.append(a)\n#         end_logits.append(a+len(train['selected_text'].iloc[i]))\n    ","execution_count":108,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess(train,'train')","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd058db8aded4bdab4203bbfc61554d2"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(train['attn_masks'][29])\n","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"(117,)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Train "},{"metadata":{"trusted":true},"cell_type":"code","source":"class Loader:\n    def __init__(self,data):\n        self.data=data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self,idx):\n        data= {\n                'ids':torch.tensor(self.data['token_ids'].iloc[idx],dtype=torch.long),\n                'token_type_ids':torch.tensor(self.data['token_type'].iloc[idx],dtype=torch.long),\n                'attn_masks':torch.tensor(self.data['attn_masks'].iloc[idx],dtype=torch.long),\n                'start':torch.tensor(self.data['start'].iloc[idx],dtype=torch.float),\n                'end':torch.tensor(self.data['end'].iloc[idx],dtype=torch.float)\n#                 'ids':torch.tensor(self.data['token_ids'][idx],dtype=torch.long),\n        }\n        return data\n        ","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device='cuda'\nelse:\n    device='cpu'","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits,end_logits,start_pos,end_pos):\n    f=nn.CrossEntropyLoss()\n    loss1=f(start_logits,torch.argmax(start_pos,dim=-1))\n    loss2=f(end_logits,torch.argmax(end_pos,dim=-1))\n    return loss1+loss2","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def training(model,train_data,device,optimizer):\n    total_loss=0\n    model.train()\n    train_data=tqdm(train_data,total=len(train_data))\n    for i,train_data1 in enumerate(train_data):\n        optimizer.zero_grad()\n        start,end=model(train_data1['ids'].to(device),train_data1['attn_masks'].to(device),train_data1['token_type_ids'].to(device))\n        loss_=loss_fn(start,end,train_data1['start'].to(device),train_data1['end'].to(device))\n        loss_.backward()\n        optimizer.step()\n        total_loss+=loss_.item()\n    return total_loss\n\ndef evaluation(model,eval_data,device,optimizer):\n    model.eval()\n    start_logits=[]\n    end_logits=[]\n    total_loss=0\n    with torch.no_grad():\n        for eval_data1 in eval_data:\n            optimizer.zero_grad()\n            start,end=model(eval_data1['ids'].to(device),eval_data1['attn_masks'].to(device),\n                            eval_data1['token_type_ids'].to(device))\n#             loss_=loss(start,end,eval_data1['start'].to(device),\n#                        eval_data1['end'].to(device))\n#             total_loss+=loss_.item()\n            \n            start=(start.cpu().detach().numpy())\n            end=(end.cpu().detach().numpy())\n            start=np.argmax(start,axis=-1)\n            end=np.argmax(end,axis=-1)\n            start_logits.append(start)\n            end_logits.append(end)\n    return total_loss,start_logits,end_logits,eval_data\n","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def train_fn(train):\n    \ntrain,test=train_test_split(train,test_size=0.2)\ntrain_data=Loader(train)\ntest_data=Loader(test)\n#     print(train['text'].iloc[0])\n#     print(train['selected_text'].iloc[0])\ntrain_data=torch.utils.data.DataLoader(train_data,batch_size=16,shuffle=True)\ntest_data=torch.utils.data.DataLoader(test_data,batch_size=16,shuffle=False)\nepochs=15\nmodel=QAModel()\nmodel.to(device)\noptimizer=optim.Adam(model.parameters(),lr=1e-5)\nfor _ in range(epochs):\n    loss=training(model,train_data,device,optimizer)\n    print(loss)\ntorch.save(model.state_dict(),'bertmodel.pkl')\n#     for train_data1 in train_data:\n#         start1,start2=model(train_data1['ids'],train_data1['attn_masks'],train_data1['token_type_ids'])\n#         break\n#     return start1,start2,train_data1['start'],train_data1['end']\n# a,b,c,d=train_fn(train)\n# train_fn(train)","execution_count":80,"outputs":[{"output_type":"stream","text":"Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c2a62cdce4402a8953ad3b2037a4a8"}},"metadata":{}},{"output_type":"stream","text":"\n2330.219277024269\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99dfa8d298004dd9ac4269f574efe4ea"}},"metadata":{}},{"output_type":"stream","text":"\n1671.6843252182007\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbca317928cb4257a4b38d8d09f12180"}},"metadata":{}},{"output_type":"stream","text":"\n1436.8315666913986\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b869b864452445b590ea30f232161473"}},"metadata":{}},{"output_type":"stream","text":"\n1238.4238522052765\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b0bd620bcd4f5dbc608c9d62eefc3f"}},"metadata":{}},{"output_type":"stream","text":"\n1042.2496291399002\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9a156fef4ce49b892b07d0d0f9284ee"}},"metadata":{}},{"output_type":"stream","text":"\n873.5108373761177\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16495e512c714cf29919f9c134880990"}},"metadata":{}},{"output_type":"stream","text":"\n706.7730885744095\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1981a700a3247aa9b519723af8668b3"}},"metadata":{}},{"output_type":"stream","text":"\n562.1287714540958\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b6392cbb38454e921d07a1ac205f5f"}},"metadata":{}},{"output_type":"stream","text":"\n457.0490790605545\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f2373e4ef446b5bec40f88d0d03ae4"}},"metadata":{}},{"output_type":"stream","text":"\n357.32158890366554\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e03223a8aef48edacbd4f54a14d31f2"}},"metadata":{}},{"output_type":"stream","text":"\n294.252699457109\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cb4937b5fb64c1482527057b6044f35"}},"metadata":{}},{"output_type":"stream","text":"\n255.48266354203224\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fbdf3aadfc4496599431e85fc91b26a"}},"metadata":{}},{"output_type":"stream","text":"\n208.87947703152895\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20348ab7829948b8a236fca6d1f64b7a"}},"metadata":{}},{"output_type":"stream","text":"\n188.66268216073513\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=563.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0fc570e44ca45a1a20952352a1f2219"}},"metadata":{}},{"output_type":"stream","text":"\n157.11604451015592\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=QAModel()\nmodel.state_dict(torch.load('./bertmodel.pkl'))\nmodel.to(device)\npreprocess(test,'test')\ntest_data=Loader(test)\ntest_data=torch.utils.data.DataLoader(test_data,batch_size=16,shuffle=False)\nloss,start_idx,end_idx,data=evaluation(model,test_data,device,optimizer)","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'QAModel' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ae3597fda314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQAModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./bertmodel.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'QAModel' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ns1=np.concatenate(start_idx)\ne1=np.concatenate(end_idx)\ncount=0\nfor i in range(len(s1)):\n    if(s1[i]>e1[i]):\n        count+=1\ncount","execution_count":110,"outputs":[{"output_type":"execute_result","execution_count":110,"data":{"text/plain":"932"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"e1.shape","execution_count":111,"outputs":[{"output_type":"execute_result","execution_count":111,"data":{"text/plain":"(2251,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"selec_text=[]\ndef jaccard_score(start,end):\n    score=0\n    for i in range(len(start)):\n#         if start[i]>=end[i]:\n#             selec_text.append(test['text'].iloc[i])\n#         else:\n        selec_text.append(test['text'].iloc[i][start[i]:end[i]])\njaccard_score(s1,e1)            \nsample['selected_text']=selec_text\nsample.to_csv('sample_submission.csv',header=None,index=None)","execution_count":84,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Length of values (2251) does not match length of index (3518)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-84-279fa70f54f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mselec_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselec_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3043\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3118\u001b[0m         \"\"\"\n\u001b[1;32m   3119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3120\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3121\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3767\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3768\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3770\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         raise ValueError(\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;34m\"does not match length of index \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Length of values (2251) does not match length of index (3518)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"k=19\nprint(test['text'].iloc[k],test['selected_text'].iloc[k],s1[k],e1[k],np.argmax(test['start'].iloc[k]),\n     np.argmax(test['end'].iloc[k]),test['sentiment'].iloc[k])","execution_count":79,"outputs":[{"output_type":"stream","text":"Just got my heart ripped out  i love you guys Just got my heart ripped out  i love you guys 15 0 1 10 positive\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,value in enumerate(data):\n    print(len(samp),value)\n    if i==3:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.argmax(test['start'].iloc[i]),np.argmax(test['end'].iloc[i]))\nprint(np.argmax(s[0][i]),np.argmax(e[0][i]))\ndata{'ids'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=np.random.rand((3))\na=[3,4,5]\nb=[0,0,1]","execution_count":68,"outputs":[{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"array([0.14084189, 0.80429543, 0.92213003])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f=torch.nn.BCEWithLogitsLoss()\n# z=f(a[0],c[0])\n# z\n# max(a[0])\noutput = f(a,c)\noutput","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = target.squeeze()\ntarget","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = torch.ones(3, 5, requires_grad=True)\ntarget.shape\ntarget=target.squeeze(dim=1)\ntarget.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test=train_test_split(train,test_size=0.2)\ntrain_data=Loader(train)\ntest_data=Loader(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['token_ids']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=train['text'].iloc[6]\nb=train['selected_text'].iloc[6]\ntokenizer=transformers.BertTokenizer.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}