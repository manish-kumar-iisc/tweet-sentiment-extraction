{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis notebook shows of how we trained our 2nd level models, which are the key of our solution.\n\nIt assumes that predictions were already generated using transformers, and converted to character level with the following function :"},{"metadata":{"trusted":true},"cell_type":"code","source":"def token_level_to_char_level(text, offsets, preds):\n    probas_char = np.zeros(len(text))\n    for i, offset in enumerate(offsets):\n        if offset[0] or offset[1]: # remove padding and sentiment\n            probas_char[offset[0]:offset[1]] = preds[i]\n    \n    return probas_char","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initialization"},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"import re\nimport os\nimport gc\nimport time\nimport torch\nimport pickle\nimport string\nimport random\nimport warnings\nimport datetime\nimport itertools\nimport tokenizers\nimport numpy as np\nimport transformers\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom torch.utils.data import *\nfrom torch.nn import functional as F\n\n\n# from torchcontrib.optim import SWA\nfrom torch.utils.data.sampler import *\nfrom torch.utils.data import DataLoader\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.preprocessing.sequence import pad_sequences\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nwarnings.filterwarnings(\"ignore\")","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nSEED = 2020\nseed_everything(SEED)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = \"../input/tweet-sentiment-extraction/\"\nPKL_PATH = \"../input/tweet-char-lvl-preds/\"\n\nK = 5\nNUM_WORKERS = 4","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(DATA_PATH + 'test.csv').fillna('')\ndf_test['selected_text'] = ''\nsub = pd.read_csv(DATA_PATH + 'sample_submission.csv')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELS = [\n    ('bert-base-uncased-'),\n    (\"distil_\"),\n    (\"large_\")\n]\n\nadd_spaces_to = [\"bert_\", 'bertweet-']","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Retrieveing 1st level model outputs"},{"metadata":{},"cell_type":"markdown","source":"## Test predictions"},{"metadata":{},"cell_type":"markdown","source":"For inference on the private set, I use some of the first level scripts to retrieve the models. I only use a few models here, for faster inference time."},{"metadata":{},"cell_type":"markdown","source":"### DistilRoberta"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"%%time\n!python ../input/distil-roberta/infer.py","execution_count":7,"outputs":[{"output_type":"stream","text":"Traceback (most recent call last):\n  File \"../input/distil-roberta/infer.py\", line 88, in <module>\n    run()\n  File \"../input/distil-roberta/infer.py\", line 26, in run\n    model.to(device)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 425, in to\n    return self._apply(convert)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 223, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 423, in convert\n    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/cuda/__init__.py\", line 196, in _lazy_init\n    _check_driver()\n  File \"/opt/conda/lib/python3.6/site-packages/torch/cuda/__init__.py\", line 94, in _check_driver\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\nAssertionError: Torch not compiled with CUDA enabled\nCPU times: user 229 ms, sys: 61.9 ms, total: 291 ms\nWall time: 13.9 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Roberta"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n!python ../input/roberta-base/infer.py","execution_count":8,"outputs":[{"output_type":"stream","text":"Traceback (most recent call last):\n  File \"../input/roberta-base/infer.py\", line 88, in <module>\n    run()\n  File \"../input/roberta-base/infer.py\", line 26, in run\n    model.to(device)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 425, in to\n    return self._apply(convert)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 223, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 423, in convert\n    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n  File \"/opt/conda/lib/python3.6/site-packages/torch/cuda/__init__.py\", line 196, in _lazy_init\n    _check_driver()\n  File \"/opt/conda/lib/python3.6/site-packages/torch/cuda/__init__.py\", line 94, in _check_driver\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\nAssertionError: Torch not compiled with CUDA enabled\nCPU times: user 289 ms, sys: 74.8 ms, total: 363 ms\nWall time: 16.2 s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Bert-base"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\n!python ../input/tweet-inference-scripts/inference_bert_base.py","execution_count":8,"outputs":[{"output_type":"stream","text":"\n   -  Doing inference for bert-base-neutral\n\n\n -> Loading weights from ../input/tweet-checkpoints-2/bert-base-uncased_2020-06-03 1946_1.pt\n\n\n -> Loading weights from ../input/tweet-checkpoints-2/bert-base-uncased_2020-06-03 1946_2.pt\n\n\n -> Loading weights from ../input/tweet-checkpoints-2/bert-base-uncased_2020-06-03 1946_3.pt\n\n\n -> Loading weights from ../input/tweet-checkpoints-2/bert-base-uncased_2020-06-03 1946_4.pt\n\n\n -> Loading weights from ../input/tweet-checkpoints-2/bert-base-uncased_2020-06-03 1946_5.pt\n\nCPU times: user 1.02 s, sys: 267 ms, total: 1.29 s\nWall time: 1min 18s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Retrieve everything"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def create_input_data(models):\n    char_pred_test_starts = []\n    char_pred_test_ends = []\n    i=0\n    for model, _ in models:\n        with open(PKL_PATH+model + 'char_pred_test_start.pkl', \"rb\") as fp:   #Pickling\n            probas = pickle.load(fp)  \n    #             print(probas[0])\n            if model in add_spaces_to:\n                probas = [np.concatenate([np.array([0]), p]) for p in probas]\n    #             print(np.shape(probas[0]))\n    #             print(probas[0])\n            char_pred_test_starts.append(probas)\n    #             print(np.shape(char_pred_test_starts[0][0]))\n\n        with open(PKL_PATH+model + 'char_pred_test_end.pkl', \"rb\") as fp:   #Pickling\n            probas = pickle.load(fp)\n\n            if model in add_spaces_to:\n                probas = [np.concatenate([np.array([0]), p]) for p in probas]\n\n            char_pred_test_ends.append(probas)\n\n    char_pred_test_start = [np.concatenate([char_pred_test_starts[m][i][:, np.newaxis] for m in range(len(models))], \n                                           1) for i in range(len(char_pred_test_starts[0]))]\n\n    char_pred_test_end = [np.concatenate([char_pred_test_ends[m][i][:, np.newaxis] for m in range(len(models))], \n                                         1) for i in range(len(char_pred_test_starts[0]))]\n\n    return char_pred_test_start, char_pred_test_end","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"char_pred_test_start, char_pred_test_end = create_input_data(MODELS)","execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'MODELS' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-5a6d91d7d90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchar_pred_test_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_pred_test_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_input_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'MODELS' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(char_pred_test_start[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reorder(order_source, order_target, preds):\n#     assert len(order_source) == len(order_target) and len(order_target) == len(preds)\n    order_source = list(order_source)\n    new_preds = []\n    for tgt_idx in order_target:\n        new_idx = order_source.index(tgt_idx)\n        new_preds.append(preds[new_idx])\n        \n    return new_preds\n\n\ndf_train = pd.read_csv(DATA_PATH + 'train.csv').dropna().reset_index(drop=True)\ndf_train = df_train.sample(frac=1, random_state=SEED).reset_index(drop=True)\norder_t = list(df_train['textID'].values)\n\ndf_train = pd.read_csv(DATA_PATH + 'train.csv').dropna()\ndf_train = df_train.sample(frac=1, random_state=50898).reset_index(drop=True)\norder_hk = list(df_train['textID'].values)\n\nORDERS = {\n    'theo': order_t,\n    'hk': order_hk,\n}","execution_count":82,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"char_pred_oof_starts = []\nchar_pred_oof_ends = []\n\nfor model, author in tqdm(MODELS):\n    with open(PKL_PATH + model + 'char_pred_oof_start.pkl', \"rb\") as fp:   #Pickling\n        probas = pickle.load(fp)\n        \n        if author != 'hk':\n            probas = reorder(ORDERS[author], ORDERS['hk'], probas)\n        \n        if model in add_spaces_to:\n            probas = [np.concatenate([np.array([0]), p]) for p in probas]\n            \n        char_pred_oof_starts.append(probas)\n\n    with open(PKL_PATH + model + 'char_pred_oof_end.pkl', \"rb\") as fp:   #Pickling\n        probas = pickle.load(fp)\n        \n        if model in add_spaces_to:\n            probas = [np.concatenate([np.array([0]), p]) for p in probas]\n        \n        if author != 'hk':\n            probas = reorder(ORDERS[author], ORDERS['hk'], probas)\n            \n        char_pred_oof_ends.append(probas)","execution_count":83,"outputs":[{"output_type":"stream","text":"100%|██████████| 5/5 [00:47<00:00,  9.56s/it]\n","name":"stderr"}]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n_models = len(MODELS)\n\nchar_pred_oof_start = [np.concatenate([char_pred_oof_starts[m][i][:, np.newaxis] for m in range(n_models)], \n                                      1) for i in range(len(df_train))]\n\nchar_pred_oof_end = [np.concatenate([char_pred_oof_ends[m][i][:, np.newaxis] for m in range(n_models)], \n                                      1) for i in range(len(df_train))]","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = {\n    'test_start': np.array(char_pred_test_start),\n    'test_end': np.array(char_pred_test_end),\n    'oof_start': np.array(char_pred_oof_start),\n    'oof_end': np.array(char_pred_oof_end),\n}\n\nmodel_names = [a + ' : ' + m for m, a in MODELS]\ncombs = [model_names]\n\nprint('Using models : ', combs)","execution_count":85,"outputs":[{"output_type":"stream","text":"Using models :  [['theo : bert-base-uncased-', 'theo : bert-wwm-neutral-', 'hk : roberta-', 'hk : distil_', 'hk : large_']]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Text Data"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=None, char_level=True, oov_token='UNK', lower=True)\ntokenizer.fit_on_texts(df_train['text'].values)\nlen_voc = len(tokenizer.word_index) + 1\n\nX_train = tokenizer.texts_to_sequences(df_train['text'].values)\nX_test = tokenizer.texts_to_sequences(df_test['text'].values)","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.shape(X_train[1])\n# X_train[1]","execution_count":87,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_start_end_string(text, selected_text):\n    len_selected_text = len(selected_text)\n    idx_start, idx_end = 0, 0\n    \n    candidates_idx = [i for i, e in enumerate(text) if e == selected_text[0]]\n    for idx in candidates_idx:\n        if text[idx : idx + len_selected_text] == selected_text:\n            idx_start = idx\n            idx_end = idx + len_selected_text\n            break\n    assert text[idx_start: idx_end] == selected_text, f'\"{text[idx_start: idx_end]}\" instead of \"{selected_text}\" in \"{text}\"'\n\n    char_targets = np.zeros(len(text))\n    char_targets[idx_start: idx_end] = 1\n    \n    return idx_start, idx_end","execution_count":88,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\nstart_idx = []\nend_idx = []\nseg_label = np.zeros((len(df), 150))\nfor i, (text, sel_text) in enumerate(zip(df['text'].values, df['selected_text'].values)):\n    print(sel_text)\n    start, end = get_start_end_string(text, sel_text.strip())\n    start_idx.append(start)\n    end_idx.append(end)\n    seg_label[i, start:end] = 1\n    break","execution_count":104,"outputs":[{"output_type":"stream","text":"I`d have responded, if I were going\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_label[0]","execution_count":109,"outputs":[{"output_type":"execute_result","execution_count":109,"data":{"text/plain":"array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetCharDataset(Dataset):\n    def __init__(self, df, X, start_probas, end_probas, n_models=1, max_len=150, train=True):\n        self.max_len = max_len\n\n        self.X = pad_sequences(X, maxlen=max_len, padding='post', truncating='post')\n        \n        self.start_probas = np.zeros((len(df), max_len, n_models), dtype=float)\n        for i, p in enumerate(start_probas):\n            len_ = min(len(p), max_len)\n            self.start_probas[i, :len_] = p[:len_]\n\n        self.end_probas = np.zeros((len(df), max_len, n_models), dtype=float)\n        for i, p in enumerate(end_probas):\n            len_ = min(len(p), max_len)\n            self.end_probas[i, :len_] = p[:len_]\n            \n        self.sentiments_list = ['positive', 'neutral', 'negative']\n        \n        self.texts = df['text'].values\n        self.selected_texts = df['selected_text'].values if train else [''] * len(df)\n        self.sentiments = df['sentiment'].values\n        self.sentiments_input = [self.sentiments_list.index(s) for s in self.sentiments]\n        \n        # Targets\n        self.seg_label = np.zeros((len(df), max_len))\n        \n        if train:\n            self.start_idx = []\n            self.end_idx = []\n            for i, (text, sel_text) in enumerate(zip(df['text'].values, df['selected_text'].values)):\n                start, end = get_start_end_string(text, sel_text.strip())\n                self.start_idx.append(start)\n                self.end_idx.append(end)\n                self.seg_label[i, start:end] = 1\n        else:\n            self.start_idx = [0] * len(df)\n            self.end_idx = [0] * len(df)\n        \n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return {\n            'ids': torch.tensor(self.X[idx], dtype=torch.long),\n            'probas_start': torch.tensor(self.start_probas[idx]).float(),\n            'probas_end': torch.tensor(self.end_probas[idx]).float(),\n            'target_start': torch.tensor(self.start_idx[idx], dtype=torch.long),\n            'target_end': torch.tensor(self.end_idx[idx], dtype=torch.long),\n            'text': self.texts[idx],\n            'selected_text': self.selected_texts[idx],\n            'sentiment': self.sentiments[idx],\n            'sentiment_input': torch.tensor(self.sentiments_input[idx]),\n            'seg_label': torch.tensor(self.seg_label[idx])\n        }","execution_count":89,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss\n- We use the cross-entropy loss with label smoothing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# one_hot = torch.zeros_like(pred).scatter(1, truth.view(-1, 1), 1)\n# one_hot_ = one_hot.clone()","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ce_loss(\n    pred, truth, smoothing=False, neighbour_smoothing=False, trg_pad_idx=-1, eps=0.1\n):\n    truth = truth.contiguous().view(-1)\n\n    one_hot = torch.zeros_like(pred).scatter(1, truth.view(-1, 1), 1)\n    one_hot_ = one_hot.clone()\n\n    if smoothing:\n        n_class = pred.size(1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n\n        if neighbour_smoothing:\n            n = 1\n            for i in range(1, n):\n                one_hot[:, :-i] += ((n - i) * eps) * one_hot_[:, i:]\n                one_hot[:, i:] += ((n - i) * eps) * one_hot_[:, :-i]\n            one_hot = one_hot / one_hot.sum(1, keepdim=True)\n\n    loss = -one_hot * F.log_softmax(pred, dim=1)\n\n    if trg_pad_idx >= 0:\n        loss = loss.sum(dim=1)\n        non_pad_mask = truth.ne(trg_pad_idx)\n        loss = loss.masked_select(non_pad_mask)\n\n    return loss.sum()","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions, config):\n\n    bs = start_logits.size(0)\n\n    start_loss = ce_loss(\n        start_logits,\n        start_positions,\n        smoothing=config[\"smoothing\"],\n        eps=config[\"eps\"],\n        neighbour_smoothing=config[\"neighbour_smoothing\"],\n    )\n\n    end_loss = ce_loss(\n        end_logits,\n        end_positions,\n        smoothing=config[\"smoothing\"],\n        eps=config[\"eps\"],\n        neighbour_smoothing=config[\"neighbour_smoothing\"],\n    )\n\n    total_loss = start_loss + end_loss\n\n    return total_loss / bs","execution_count":92,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metric"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def jaccard_from_logits_string(data, start_logits, end_logits):\n    \n    n = start_logits.size(0)\n    score = 0\n\n    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n\n    for i in range(n):\n        start_idx = np.argmax(start_logits[i])\n        end_idx = np.argmax(end_logits[i])\n        text = data[\"text\"][i]\n        pred = text[start_idx: end_idx]\n\n        score += jaccard(data[\"selected_text\"][i], pred)\n\n    return score","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    try:\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    except:\n        return 0","execution_count":94,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def predict(model, dataset, batch_size=32):\n    model.eval()\n    start_probas = []\n    end_probas = []\n\n    loader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n    )\n\n    with torch.no_grad():\n        for data in loader:\n            start_logits, end_logits = model(\n                data[\"ids\"].cuda(), \n                data['sentiment_input'].cuda(), \n                data['probas_start'].cuda(), \n                data['probas_end'].cuda()\n            )\n\n            start_probs = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n            end_probs = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n\n            for s, e in zip(start_probs, end_probs):\n                start_probas.append(list(s))\n                end_probas.append(list(e))\n\n    return start_probas, end_probas","execution_count":95,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def fit(\n    model,\n    train_dataset,\n    val_dataset,\n    loss_config,\n    epochs=5,\n    swa_first_epoch=5,\n    batch_size=8,\n    acc_steps=1,\n    weight_decay=0,\n    warmup_prop=0.0,\n    lr=5e-4,\n    cp=False,\n    use_len_sampler=True,\n):\n    best_jac = 0\n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, num_workers=NUM_WORKERS\n    )\n\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n    )\n\n    optimizer = Adam(model.parameters(), lr=lr) #, betas=(0.5, 0.999))\n    optimizer = SWA(optimizer)\n\n    n_steps = float(epochs * len(train_loader)) / float(acc_steps)\n    num_warmup_steps = int(warmup_prop * n_steps)\n\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps, n_steps\n    )\n\n    total_steps = 0\n    for epoch in range(epochs):\n        model.train()\n        start_time = time.time()\n\n        optimizer.zero_grad()\n        avg_loss = 0\n\n        for step, data in enumerate(train_loader):\n            total_steps += 1\n            start_logits, end_logits = model(\n                data[\"ids\"].cuda(), \n                data['sentiment_input'].cuda(), \n                data['probas_start'].cuda(), \n                data['probas_end'].cuda()\n            )\n            \n            loss = loss_fn(\n                start_logits,\n                end_logits,\n                data[\"target_start\"].cuda(),\n                data[\"target_end\"].cuda(),\n                config=loss_config,\n            )\n\n            avg_loss += loss.item() / len(train_loader)\n            loss.backward()\n\n            if (step + 1) % acc_steps == 0:\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n        model.eval()\n        avg_val_loss = 0.0\n        val_jac = 0.0\n\n        if epoch + 1 >= swa_first_epoch:\n            optimizer.update_swa()\n            optimizer.swap_swa_sgd()\n\n        with torch.no_grad():\n            for data in val_loader:\n                \n                start_logits, end_logits = model(\n                    data[\"ids\"].cuda(), \n                    data['sentiment_input'].cuda(), \n                    data['probas_start'].cuda(), \n                    data['probas_end'].cuda()\n                )\n\n                loss = loss_fn(\n                    start_logits.detach(),\n                    end_logits.detach(),\n                    data[\"target_start\"].cuda().detach(),\n                    data[\"target_end\"].cuda().detach(),\n                    config=loss_config,\n                )\n\n                avg_val_loss += loss.item() / len(val_loader)\n\n                val_jac += jaccard_from_logits_string(data, start_logits, end_logits) / len(\n                    val_dataset\n                )\n        \n        if epoch + 1 >= swa_first_epoch:\n            optimizer.swap_swa_sgd()\n            \n        if val_jac >= best_jac and cp:\n            save_model_weights(model, \"checkpoint.pt\", verbose=0)\n            best_jac = val_jac\n\n        dt = time.time() - start_time\n        lr = scheduler.get_lr()[0]\n        print(f\"Epoch {epoch + 1}/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t\", end=\"\")\n        print(\n            f\"loss={avg_loss:.3f} \\t val_loss={avg_val_loss:.3f} \\t val_jaccard={val_jac:.4f}\"\n        )\n\n    del loss, data, avg_val_loss, avg_loss, train_loader, val_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    if epoch >= swa_first_epoch: # back to swa weights\n        optimizer.swap_swa_sgd()\n\n    return best_jac if cp else val_jac","execution_count":97,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models\nWe have three models : \n- A RNN\n- A 1D-CNN\n- A Wavenet"},{"metadata":{},"cell_type":"markdown","source":"## Modules"},{"metadata":{},"cell_type":"markdown","source":"## RNN"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class TweetCharModel(nn.Module):\n    def __init__(self, len_voc, use_msd=True,\n                 embed_dim=64, lstm_dim=64, char_embed_dim=32, sent_embed_dim=32, ft_lstm_dim=32, n_models=1):\n        super().__init__()\n        self.use_msd = use_msd\n        \n        self.char_embeddings = nn.Embedding(len_voc, char_embed_dim)\n        self.sentiment_embeddings = nn.Embedding(3, sent_embed_dim)\n        \n        self.proba_lstm = nn.LSTM(n_models * 2, ft_lstm_dim, batch_first=True, bidirectional=True)\n        \n        self.lstm = nn.LSTM(char_embed_dim + ft_lstm_dim * 2 + sent_embed_dim, lstm_dim, batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(lstm_dim * 2, lstm_dim, batch_first=True, bidirectional=True)\n\n        self.logits = nn.Sequential(\n            nn.Linear(lstm_dim *  4, lstm_dim),\n            nn.ReLU(),\n            nn.Linear(lstm_dim, 2),\n        )\n        \n        self.high_dropout = nn.Dropout(p=0.5)\n    \n    def forward(self, tokens, sentiment, start_probas, end_probas):\n        bs, T = tokens.size()\n        \n        probas = torch.cat([start_probas, end_probas], -1)\n        probas_fts, _ = self.proba_lstm(probas)\n\n        char_fts = self.char_embeddings(tokens)\n        \n        sentiment_fts = self.sentiment_embeddings(sentiment).view(bs, 1, -1)\n        sentiment_fts = sentiment_fts.repeat((1, T, 1))\n        \n        features = torch.cat([char_fts, sentiment_fts, probas_fts], -1)\n        features, _ = self.lstm(features)\n        features2, _ = self.lstm2(features)\n        \n        features = torch.cat([features, features2], -1)\n        \n        if self.use_msd and self.training:\n            logits = torch.mean(\n                torch.stack(\n                    [self.logits(self.high_dropout(features)) for _ in range(5)],\n                    dim=0,\n                    ),\n                dim=0,\n            )\n        else:\n            logits = self.logits(features)\n\n        start_logits, end_logits = logits[:, :, 0], logits[:, :, 1]\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# $k$-fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"k=5\nsplits = list(StratifiedKFold(n_splits=k, random_state=23).split(X=df_train, y=df_train['sentiment']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold(config, df_train, df_test, X_train, X_test, preds, len_voc, k=5, seed=42, save=True, model_name='model'):\n    time = str(datetime.datetime.now())[:16]\n    score = 0\n    splits = list(StratifiedKFold(n_splits=k, random_state=seed).split(X=df_train, y=df_train['sentiment']))\n    \n    pred_oof = [[[], []] for i in range(len(df_train))]\n    pred_tests = [] \n    \n    test_dataset = TweetCharDataset(\n        df_test, X_test, preds['test_start'], preds['test_end'], \n        max_len=config.max_len_val, train=False, n_models=config.n_models\n    )\n    \n    for i, (train_idx, val_idx) in enumerate(splits):\n        print(f\"\\n-------------   Fold {i + 1}  -------------\")\n        seed_everything(seed + i)\n\n        if config.model == 'rnn':\n            model = TweetCharModel(\n                len_voc,\n                use_msd=config.use_msd, \n                n_models=config.n_models,   \n                lstm_dim=config.lstm_dim,\n                ft_lstm_dim=config.ft_lstm_dim,\n                char_embed_dim=config.char_embed_dim,\n                sent_embed_dim=config.sent_embed_dim,\n            ).cuda()\n        elif config.model == 'cnn':\n            model = ConvNet(\n                len_voc,\n                use_msd=config.use_msd, \n                n_models=config.n_models,  \n                use_bn=config.use_bn,\n                cnn_dim=config.cnn_dim,\n                proba_cnn_dim=config.proba_cnn_dim,\n                char_embed_dim=config.char_embed_dim,\n                sent_embed_dim=config.sent_embed_dim,\n                kernel_size=config.kernel_size,\n            ).cuda()\n        else:\n            model = WaveNet(\n                len_voc,\n                use_msd=config.use_msd, \n                n_models=config.n_models,  \n                use_bn=config.use_bn,\n                cnn_dim=config.cnn_dim,\n                proba_cnn_dim=config.proba_cnn_dim,\n                char_embed_dim=config.char_embed_dim,\n                sent_embed_dim=config.sent_embed_dim,\n                kernel_size=config.kernel_size,\n                dilations=config.dilations, \n            ).cuda()\n        \n        model.zero_grad()\n\n        train_dataset = TweetCharDataset(\n            df_train.iloc[train_idx],\n            X_train[train_idx],\n            preds['oof_start'][train_idx],\n            preds['oof_end'][train_idx],\n            max_len=config.max_len,\n            n_models=config.n_models,\n        )\n#         print()\n        val_dataset = TweetCharDataset(\n            df_train.iloc[val_idx], \n            X_train[val_idx], \n            preds['oof_start'][val_idx], \n            preds['oof_end'][val_idx],\n            max_len=config.max_len_val,\n            n_models=config.n_models,\n        )\n        \n        print('\\n- Training \\n')\n\n        fold_score = fit(\n            model, \n            train_dataset, \n            val_dataset, \n            config.loss_config,\n            epochs=config.epochs, \n            batch_size=config.batch_size, \n            lr=config.lr, \n            warmup_prop=config.warmup_prop,\n            swa_first_epoch=config.swa_first_epoch,\n            use_len_sampler=config.use_len_sampler,\n            cp=False\n        )\n        \n        score += fold_score / k\n\n        print('\\n- Predicting ')\n\n        pred_val_start, pred_val_end = predict(model, val_dataset, batch_size=config.batch_size_val)\n        for j, idx in enumerate(val_idx):\n            pred_oof[idx] = [pred_val_start[j], pred_val_end[j]]\n        \n        pred_test = predict(model, test_dataset, batch_size=config.batch_size_val)\n        pred_tests.append(pred_test)            \n\n        del model, train_dataset, val_dataset\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    print(f'\\n Local CV jaccard is {score:.4f}')\n    return pred_oof, pred_tests","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configs"},{"metadata":{},"cell_type":"markdown","source":"## RNN"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class ConfigRNN:\n    model = 'rnn'\n    n_models = len(MODELS)\n    \n    # Texts\n    max_len = 150\n    max_len_val = 150\n    \n    # Architecture\n    sent_embed_dim = 16 # 32 works as well\n    char_embed_dim = 8\n    ft_lstm_dim = 16\n    \n    lstm_dim = 64\n    use_msd = True\n    \n    # Loss function\n    loss_config = {\n        \"smoothing\": True,\n        \"neighbour_smoothing\": False,\n        \"eps\": 0.1,\n        \"use_dist_loss\": False,\n        \"dist_loss_weight\": 1,\n    }\n    \n    # Training\n    use_len_sampler = False\n    \n    batch_size = 128\n    batch_size_val = 512\n\n    epochs = 1\n    swa_first_epoch = 5\n    lr = 5e-3\n    warmup_prop = 0.\n\n    # Post-processing\n    remove_neutral = False\n    \n    pl_confidence = 0.35","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"configs = [ConfigWav(), ConfigRNN(), ConfigCNN()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_oofs = []\npred_tests = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, comb in enumerate(combs):        \n    print('#' * 80)\n    print(f' -> Combination {idx + 1}/{len(combs)} : \\n {\" / \".join(list(comb))} ')\n    print('#' * 80, \"\\n\")\n    used = [model_names.index(c) for c in comb]\n    \n    used_preds = {}\n    for key in preds.keys():\n        used_preds[key] = np.array([preds[key][i][:, used] for i in range(len(preds[key]))])\n    \n    for config in configs:\n        \n        print(f' -> Training {config.model.upper()}')\n        \n        config.n_models = len(used)\n        pred_oof, pred_test = k_fold(config, df_train, df_test, np.array(X_train), np.array(X_test), used_preds, len_voc, \n                                      k=K, seed=SEED, model_name='wavenet_0_0')\n        \n        pred_oofs.append(pred_oof)\n        pred_tests.append(pred_test)\n        \n        print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrieving predictions"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def string_from_preds_char_level(dataset, preds, test=False, remove_neutral=False, uncensored=False, cleaned=False):\n    selected_texts = []\n    n_models = len(preds)\n\n    for idx in range(len(dataset)):\n        data = dataset[idx]\n\n        if test:\n            start_probas = np.mean([preds[i][0][idx] for i in range(n_models)], 0)\n            end_probas = np.mean([preds[i][1][idx] for i in range(n_models)], 0)\n        else:\n            start_probas = preds[idx][0]\n            end_probas = preds[idx][1]\n\n        start_idx = np.argmax(start_probas)\n        end_idx = np.argmax(end_probas)\n\n        if end_idx < start_idx:\n            selected_text = data[\"text\"]\n        elif remove_neutral and data[\"sentiment\"] == \"neutral\":\n            selected_text = data[\"text\"]\n        else:\n            selected_text = data[\"text\"][start_idx: end_idx]\n\n        selected_texts.append(selected_text.strip())\n\n    return selected_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"config = ConfigWav()\nconfig.n_models = len(preds['oof_start'][0][0])\n\ntest_dataset = TweetCharDataset(\n    df_test, X_test, preds['test_start'], preds['test_end'], \n    max_len=config.max_len_val, train=False, n_models=config.n_models, \n)\n\ndataset = TweetCharDataset(\n    df_train, X_train, preds['test_start'], preds['test_end'], \n    max_len=config.max_len_val, train=False, n_models=config.n_models, \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_oof = (np.array(pred_oofs[0]) + np.array(pred_oofs[1]) + np.array(pred_oofs[2])) / 3\npred_test = pred_tests[0] + pred_tests[1] + pred_tests[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(pred_oof)\nnp.argmax(pred_oof[1][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_texts_oof = string_from_preds_char_level(dataset, pred_oof, test=False, remove_neutral=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = [jaccard(pred, truth) for (pred, truth) in zip(selected_texts_oof, df_train['selected_text'])]\nscore = np.mean(scores)\nprint(f'Local CV score is {score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_texts = string_from_preds_char_level(test_dataset, pred_test, test=True, remove_neutral=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(selected_texts)\nselected_texts[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['selected_text'] = selected_texts  \nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}